You are working on an existing production web app that currently lives at https://marketplace.sirdab.co, and we are migrating it to this new Replit-based app without hurting SEO.

Your task is to implement robots.txt and sitemap endpoints that:

Preserve the current URL structure

Work for any domain the app is deployed under (do not hardcode marketplace.sirdab.co)

Follow the official sitemaps.org spec

0. Understand the existing app

Inspect the project and detect the stack/framework:

If this is Next.js / Remix → use the framework’s built-in routing conventions.

If this is Express / Fastify / Flask / etc. → add explicit HTTP routes.

Do not change the existing URL structure, especially:

GET /ads/:adId → individual ad page

GET /ads/region/sa/:city → regional listing page

Assume there is some persistent storage (DB or API) for ads, with at least:

id (ad identifier)

updated_at or equivalent timestamp

published boolean (or similar) to indicate ads that should be public

Use whatever models/ORM/client the project already uses (Prisma, Supabase client, etc.).

1. Base URL handling (important: domain must be dynamic)

Wherever you construct absolute URLs in sitemaps:

Do not hardcode the domain.

Derive the base URL from the incoming request, for example:

Protocol: default to https in production if not provided.

Host: use the Host header (or framework helper like req.headers.host, request.headers.get("host"), etc.).

Then build URLs like:

baseUrl + "/ads/" + adId
baseUrl + "/ads/region/sa/" + city

Make sure this works in all environments (local, staging, production).

2. Implement GET /sitemap.xml (sitemap index)

Create a route/handler for GET /sitemap.xml that:

Sets header:

Content-Type: application/xml; charset=utf-8

Dynamically builds the base URL from the incoming request.

Returns a sitemap index XML that points to the child sitemap (for now just one):

<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <sitemap>
    <loc>https://CURRENT_DOMAIN/sitemap-0.xml</loc>
  </sitemap>
</sitemapindex>


Where https://CURRENT_DOMAIN is derived from the request (domain is not hardcoded).

3. Implement GET /sitemap-0.xml (URL sitemap)

Create a route/handler for GET /sitemap-0.xml that:

Sets header:

Content-Type: application/xml; charset=utf-8

Dynamically builds baseUrl from the incoming request.

Fetches all published ads from the database, e.g.:

Only include ads where published = true (or equivalent field).

For each ad you must have:

ad.id → to build /ads/:id

ad.updated_at (or similar) → to use as <lastmod>.

Builds a <urlset> XML according to sitemaps.org:

<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <!-- 1) Individual ads -->
  <url>
    <loc>BASE_URL/ads/AD_ID</loc>
    <lastmod>2025-01-12T13:50:31.555Z</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>

  <!-- 2) Region listing pages -->
  <url>
    <loc>BASE_URL/ads/region/sa/riyadh</loc>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <!-- etc... -->
</urlset>


Include all region listing URLs in the same sitemap:

Use exactly these paths:

/ads/region/sa/riyadh

/ads/region/sa/jeddah

/ads/region/sa/dammam

/ads/region/sa/al-khobar

/ads/region/sa/al-ahsa

/ads/region/sa/abha

/ads/region/sa/buraydah

For these region URLs, you can:

Set changefreq to daily

Set priority to 0.7

Optionally set lastmod to either:

A fixed ISO timestamp, or

The most recent updated_at from ads in that region (if easy to compute).

Ensure the resulting XML is valid and well-formed:

Proper XML header

Proper namespace: xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"

Proper escaping of special characters if any.

4. Implement GET /robots.txt

Create a route/handler for GET /robots.txt that:

Sets header:

Content-Type: text/plain; charset=utf-8

Dynamically builds baseUrl from the incoming request.

Returns plain text like:

User-agent: *
Allow: /

Sitemap: BASE_URL/sitemap.xml
Sitemap: BASE_URL/sitemap-0.xml


Where BASE_URL includes the scheme and host derived from the request.

Do not hardcode https://marketplace.sirdab.co. Always compute the base URL from the actual request so this works on any domain.

5. SEO / migration requirements to respect

While implementing this:

Do not change the existing URL structure:

/ads/:adId must continue to work and return 200.

/ads/region/sa/:city must continue to work and return 200.

Ensure:

All URLs included in the sitemap respond with HTTP 200.

No framework-level redirects or rewrites break these routes.

Make sure responses are not cached incorrectly during development. In production, it’s okay to use reasonable caching for sitemaps/robots, but it should reflect new ads over time.

6. Quality checks

After implementing:

Show me the final code snippets for:

sitemap.xml route/handler

sitemap-0.xml route/handler

robots.txt route/handler

Show example responses (as text) for:

A sample GET /sitemap.xml request.

A sample GET /sitemap-0.xml with at least 2 fake ads and all region URLs.

A sample GET /robots.txt response.

Make sure all of this is implemented idiomatically for the actual framework used in this project.